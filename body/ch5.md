### 第5章 重抽样方法 (Resampling Methods) 

本章介绍了重抽样方法，这在现代统计学中是不可或缺的工具。核心思想是从训练数据集中重复抽取样本，并在每个样本上重新拟合模型，从而获取关于拟合模型的额外信息。尽管这些方法的计算成本可能很高，但现代计算能力的进步使其变得可行。本章重点讨论了两种最常用和最重要的方法：**交叉验证 (Cross-Validation)** 和 **自助法 (Bootstrap)**。

---

### **第一部分：交叉验证 (Cross-Validation)**

交叉验证可用于估计给定统计学习方法的测试误差，从而评估其性能（模型评估）或选择合适的模型灵活性（模型选择）。其主要目的是在没有专门的测试集的情况下，利用训练数据来估计测试误差率。

#### **1. 验证集方法 (The Validation Set Approach)**

这是最简单的交叉验证策略。

* **工作原理**:
    * 将可用的观测数据随机分成两部分：一个**训练集 (training set)** 和一个**验证集 (validation set)** 或称**保留集 (hold-out set)**。
    * 在训练集上拟合模型。
    * 使用拟合好的模型对验证集中的观测数据进行预测。
    * 计算验证集上的误差（例如，对于定量响应，使用均方误差 MSE），以此作为对测试误差率的估计。

* **缺点**:
    1.  **高可变性**: 测试误差的估计值可能非常不稳定，这取决于哪些观测值被分到训练集，哪些被分到验证集。多次重复随机分割会得到不同的测试误差估计值。
    2.  **高估测试误差**: 模型仅在训练集（通常是原始数据的一部分，例如一半）上进行训练。由于统计方法在较少的数据上训练时性能较差，这可能导致验证集误差率高估了在整个数据集上拟合的模型的测试误差率。

#### **2. 留一法交叉验证 (Leave-One-Out Cross-Validation, LOOCV)**

LOOCV 旨在解决验证集方法的缺点。

* **工作原理**:
    * 将数据集分成两部分，但不是大小相当的子集，而是将单个观测值 $(x_1, y_1)$ 作为验证集，其余的 $n-1$ 个观测值作为训练集。
    * 在 $n-1$ 个训练观测值上拟合模型，并对被排除的观测值 $x_1$ 做出预测 $\hat{y}_1$。
    * 计算其均方误差 $MSE_1 = (y_1 - \hat{y}_1)^2$。
    * 对数据集中的每个观测值（共 $n$ 次）重复此过程，每次都将不同的单个观测值作为验证集。
    * 最终的LOOCV测试MSE估计值是这 $n$ 个MSE的平均值：
        $$CV_{(n)} = \frac{1}{n}\sum_{i=1}^{n}MSE_i$$

* **优点**:
    * **偏差小**: 由于每次训练集都包含 $n-1$ 个观测值（几乎是整个数据集），LOOCV 对测试误差率的估计偏差远小于验证集方法。
    * **无随机性**: 多次执行LOOCV总会得到相同的结果，因为训练/验证集的划分没有随机性。

* **缺点**:
    * **计算成本高**: 需要拟合模型 $n$ 次，如果 $n$ 很大或模型拟合很慢，这可能会非常耗时。(对于最小二乘线性回归或多项式回归，有一个快捷计算公式可以降低成本。)

#### **3. k折交叉验证 (k-Fold Cross-Validation)**

k折交叉验证是LOOCV的一种替代方案，也是实践中最常用的方法。

* **工作原理**:
    * 将观测数据集随机分成 $k$ 个大小近似相等的组，或称**折 (folds)**。
    * 将第一折作为验证集，在其余的 $k-1$ 折上拟合模型。
    * 计算在验证集上的均方误差 $MSE_1$。
    * 重复此过程 $k$ 次，每次都使用不同的一折作为验证集。
    * k折CV的估计值是这 $k$ 个MSE值的平均值：
        $$CV_{(k)} = \frac{1}{k}\sum_{i=1}^{k}MSE_i$$

* **特点**:
    * LOOCV是k折CV的一个特例，其中 $k=n$。
    * 在实践中，通常使用 $k=5$ 或 $k=10$。
    * **计算优势**: 相比需要拟合 $n$ 次的LOOCV，k折CV只需要拟合 $k$ 次，计算上更可行。
    * 其可变性通常远低于验证集方法。

#### **4. k折交叉验证的偏差-方差权衡**

选择 $k$ 的值涉及到偏差和方差之间的权衡。

* **偏差**:
    * LOOCV的训练集大小为 $n-1$，几乎与整个数据集一样大，因此其对测试误差的估计是**近似无偏的**。
    * k折CV的训练集大小为 $(k-1)n/k$，