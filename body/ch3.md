# 第3章 线性回归

## 1. 线性回归简介

线性回归是一种用于预测**定量响应变量**的监督学习方法。它历史悠久、应用广泛，并且是理解许多高级统计学习方法的基础。

本章旨在回答以下核心问题：
* 广告投入与产品销量之间是否存在关系？
* 这种关系的强度如何？
* 哪些广告媒体（如电视、广播、报纸）对销量有显著影响？
* 如何准确预测未来的销量？
* 预测变量与响应变量之间的关系是线性的吗？
* 不同广告媒体之间是否存在**协同效应**（即交互作用）？

---

## 2. 简单线性回归

简单线性回归使用**单个预测变量 X** 来预测**定量响应变量 Y**。

### 模型
其数学模型假设 X 和 Y 之间存在近似的线性关系：
$$ Y \approx \beta_0 + \beta_1 X $$
-   **$\beta_0$**：截距 (intercept)
-   **$\beta_1$**：斜率 (slope)
-   $\beta_0$ 和 $\beta_1$ 统称为**模型系数** (model coefficients) 或**参数** (parameters)。

### 系数估计
最常用的估计方法是**最小二乘法 (least squares)**。该方法旨在最小化**残差平方和 (Residual Sum of Squares, RSS)**。
$$ RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2 $$
通过微积分可以求得使 RSS 最小化的估计值：
$$ \hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} $$
$$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} $$

### 评估系数的准确性
-   **标准误 (Standard Error, SE)**：衡量系数估计值 $\hat{\beta}$ 的不确定性。SE 越小，估计越准确。
-   **置信区间 (Confidence Intervals)**：提供一个参数真实值的可能范围。例如，一个95%的置信区间意味着该区间有95%的概率包含参数的真实值。
-   **假设检验 (Hypothesis Testing)**：
    -   **原假设 $H_0$**：X 和 Y 之间没有关系，即 $\beta_1 = 0$。
    -   **备择假设 $H_a$**：X 和 Y 之间存在关系，即 $\beta_1 \neq 0$。
    -   **t-统计量**：衡量 $\hat{\beta}_1$ 偏离0的标准差倍数。$t = (\hat{\beta}_1 - 0) / SE(\hat{\beta}_1)$。
    -   **p-值**：在 $H_0$ 为真的情况下，观测到当前或更极端结果的概率。p值越小，拒绝 $H_0$ 的证据越强。

### 评估模型的准确性
-   **残差标准误 (Residual Standard Error, RSE)**：衡量模型对数据拟合的**缺失程度**，代表响应值偏离真实回归线的平均距离。
-   **R² 统计量**：衡量响应变量 Y 的总变异中，能够被预测变量 X 解释的**比例**。其值介于0和1之间，越接近1表示模型解释能力越强。在简单线性回归中，$R^2 = r^2$ (相关系数的平方)。

---

## 3. 多元线性回归

多元线性回归使用**多个预测变量** ($X_1, X_2, \dots, X_p$) 来预测 Y。

### 模型
$$ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon $$
-   $\beta_j$ 的解释是：在**保持其他所有预测变量不变**的情况下，$X_j$ 每增加一个单位，Y 的平均变化量。

### 核心问题
1.  **是否存在关系？**
    -   使用 **F-统计量** 来检验**所有**预测变量的系数是否同时为零 ($H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0$)。F-统计量远大于1且对应的p值很小，则表明至少有一个预测变量与响应相关。

2.  **哪些变量是重要的？(变量选择)**
    -   由于检查所有 $2^p$ 个可能的模型不现实，通常采用自动化方法：
        -   **前向选择 (Forward selection)**：从空模型开始，逐个添加最显著的变量。
        -   **后向选择 (Backward selection)**：从包含所有变量的模型开始，逐个移除最不显著的变量。
        -   **混合选择 (Mixed selection)**：结合以上两种方法。

3.  **模型拟合得如何？**
    -   同样使用 RSE 和 R²。但需注意，向模型中添加变量**总会**使训练集上的 R² 增加，即使该变量与响应无关。

4.  **预测的准确性**
    -   **置信区间**：用于量化**平均响应**的不确定性。
    -   **预测区间**：用于量化**单个响应**的不确定性。预测区间总是比置信区间宽，因为它包含了不可约误差 $\epsilon$。

---

## 4. 回归模型的其他注意事项

### 定性预测变量
-   对于只有两个水平的定性变量，可以创建一个 **(0, 1) 虚拟变量 (dummy variable)**。
-   对于超过两个水平的定性变量，需要创建 **k-1 个**虚拟变量，其中 k 是水平的数量。未分配虚拟变量的水平称为**基线 (baseline)**。

### 线性模型的扩展
1.  **交互作用 (Interaction Terms)**
    -   放宽**可加性假设** (即预测变量的影响是独立的)。
    -   通过在模型中添加两个预测变量的乘积项 ($X_1 \times X_2$) 来实现。
    -   一个显著的交互项表明，一个预测变量对响应的影响依赖于另一个预测变量的水平。这在市场营销中被称为**协同效应 (synergy effect)**。

2.  **非线性关系 (Polynomial Regression)**
    -   放宽**线性假设**。
    -   通过在模型中添加预测变量的多项式项 (如 $X^2, X^3$) 来拟合曲线关系。
    -   这本质上仍然是一个线性模型，因为它对变换后的变量是线性的。

### 潜在问题
1.  **非线性**：通过**残差图** (residuals vs. fitted values) 发现。若残差图呈现明显模式 (如U形)，则表明存在非线性。
2.  **误差项相关**：常见于时间序列数据。这会导致标准误被低估，使得置信区间过窄，p值过小。
3.  **异方差性 (Heteroscedasticity)**：误差项方差非恒定。在残差图中表现为**漏斗形状**。可通过对响应变量进行变换 (如 `log(Y)`) 来解决。
4.  **异常值 (Outliers)**：响应值 $y_i$ 远离模型预测值的点。
5.  **高杠杆点 (High-Leverage Points)**：预测值 $x_i$ 不寻常的点。这些点对回归线有很大影响。
6.  **共线性 (Collinearity)**：两个或多个预测变量高度相关。
    -   这会使系数估计变得不稳定，标准误增大。
    -   可通过**方差膨胀因子 (Variance Inflation Factor, VIF)** 检测。VIF > 5 或 10 表明存在问题。
    -   解决方法：移除一个相关变量或将它们组合成一个新变量。

---

## 5. 线性回归与 K-最近邻 (KNN) 的比较

| 特性         | 线性回归                                              | K-最近邻 (KNN)                                              |
| :----------- | :---------------------------------------------------- | :---------------------------------------------------------- |
| **方法类型** | **参数化 (Parametric)**：对函数形式 (线性) 做出假设。 | **非参数化 (Non-parametric)**：不对函数形式做假设，更灵活。 |
| **适用场景** | 当真实关系接近线性时，表现优异。                      | 当真实关系为非线性时，可能表现更好。                        |
| **可解释性** | 高。模型由几个系数定义，易于理解。                    | 低。没有简单的模型或系数可供解释。                          |
| **维度影响** | 在高维情况下表现相对稳健。                            | 在高维情况下性能急剧下降 (即**维度灾难**)。                 |

**结论**：
-   如果预测准确性是首要目标，且真实关系可能是非线性的，KNN值得考虑。
-   如果可解释性很重要，或者预测变量维度很高而样本量不大，线性回归通常是更好的选择。